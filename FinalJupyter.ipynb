{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4573615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\DA_Env\\lib\\site-packages\\statsmodels\\compat\\pandas.py:61: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import getpass\n",
    "import sqlalchemy as sa\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder  ##. better to use dummy from pandas \n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.feature_selection import RFE\n",
    "pd.options.display.max_rows = 50\n",
    "## Install xlrd package to load Excel files\n",
    "# conda install openpyxl\n",
    "## conda install xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613dccbd",
   "metadata": {},
   "source": [
    "## Importing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71384a3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hp_df_org = pd.read_csv('Data/house_price_df.csv')\n",
    "hp_df = hp_df_org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e1483",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba015ee1",
   "metadata": {},
   "source": [
    "The Dataset consists of 21597 Houses containing 21 features. Price is to be the predicted variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabbcb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decc53c",
   "metadata": {},
   "source": [
    "The average house has about 3 bedrooms, 2080 sqft of living area, a lot size of 15000 sqft, was built in 1970 and has a price of 540000 $. The mean and the median differ alot in the variable sqft lot, where the median is much lower then the mean. This leads to the conculusion that many houses don't have a big lot but a few house with a big lot change the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1a5e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.pairplot(hp_df) #To view remove comment. Commented to get higher performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd083e",
   "metadata": {},
   "source": [
    "The Pairplot gives first interesting insides. Obviously there are linear tendencies between variables of size like lot size, size of living room. Many variables are categorical or boolean and should be transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680e5ac",
   "metadata": {},
   "source": [
    "## Taking a closer look at certain variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea7c05",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hp_df['date'].min(), hp_df['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7890bf7",
   "metadata": {},
   "source": [
    "The date variable displays the date when the house was sold. Data is between May 2014 and May 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df['bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b73fd",
   "metadata": {},
   "source": [
    "Most houses have 2,5 bathrooms, a lot also only . A view outliers have a very high number of bathrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c93e42",
   "metadata": {},
   "source": [
    "## Location\n",
    "\n",
    "For the location the datasets offers three features. Latitute and Longitude (only useful combined) and zipcodes. Let's look at both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c592eae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " sns.scatterplot(data=hp_df, x='long', y='lat', hue = 'waterfront').set(title='Houses at waterfront')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4385a7e",
   "metadata": {},
   "source": [
    "Looking at the geographical location of the houses and displaying the lot size it is visual, that the properties at the are outskirts and the countryside get bigger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83796a34",
   "metadata": {},
   "outputs": [],
   "source": [
    " sns.scatterplot(data=hp_df, x='long', y='lat', hue = 'zipcode').set(title='Zipcode and geographical location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240be6c",
   "metadata": {},
   "source": [
    "The zipcodes are not in a logical order so the variables should not be used as a numerical variables.\n",
    "And alternative idea would be to use the travel distance from the house to the city center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = pd.read_csv('Data/distance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643851d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df['distanceM'] = distance['DistanceM'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=hp_df, x='long', y='lat', hue = 'distanceM').set(title='Geographical location and distance to citycenter by car')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c558344",
   "metadata": {},
   "source": [
    "## Year renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34be705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=hp_df, x='yr_renovated').set(title= 'Histogram of Year renovated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48680aaf",
   "metadata": {},
   "source": [
    "The year renovated displays a 0 if the house was not yet renovated. This is not useful information in a linear regression. \n",
    "\n",
    "Workaround:\n",
    "\n",
    "introduce boolean variable: Renovated: Yes/N0\n",
    "Put zero values as No  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df['renovated'] = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2bcb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_df.loc[hp_df['yr_renovated'] == 0, 'renovated'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b43371",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.loc[hp_df['yr_renovated'] != 0, 'renovated'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb56f67",
   "metadata": {},
   "source": [
    "## Basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6306aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=hp_df, x='sqft_basement')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07907b25",
   "metadata": {},
   "source": [
    "Most Houses don't have a basement, so a dummy will be introduced checking for basement or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c377a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.loc[hp_df['sqft_basement'] == 0, 'basement'] = False\n",
    "hp_df.loc[hp_df['sqft_basement'] != 0, 'basement'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c0b8d",
   "metadata": {},
   "source": [
    "## Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee487a25",
   "metadata": {},
   "source": [
    "Bin grade to have a better classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_mean = hp_df['grade'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77155c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(hp_df)):\n",
    "    if hp_df['grade'][i]<grade_mean:\n",
    "        hp_df['grade'][i] = 'below_avg'\n",
    "    else:\n",
    "        hp_df['grade'][i] = 'avg_or_better'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(hp_df['grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a3bfd",
   "metadata": {},
   "source": [
    "## Floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(hp_df['floors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df['floors'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a550e9",
   "metadata": {},
   "source": [
    "Binning floors to 2 categories: One floor and more than one floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1589017",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(hp_df)):\n",
    "    if hp_df.loc[i, 'floors'] <= 1.0:\n",
    "        hp_df.loc[i,'floors'] = 'one'\n",
    "    else: \n",
    "        hp_df.loc[i, 'floors'] = 'more_than_one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(hp_df['floors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca0b83",
   "metadata": {},
   "source": [
    "## Dropping columns:\n",
    "\n",
    "For now the columns Unnamed, id, date and long, lat will be dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = ['Unnamed: 0',\\\n",
    "            'id',\\\n",
    "            'date',\\\n",
    "           'long',\\\n",
    "            'sqft_basement',\\\n",
    "            'yr_renovated',\\\n",
    "           'lat']   \n",
    "# 'Unnamed: 0' : id from import without information\n",
    "# 'id': random or consequtive values without values\n",
    "# 'date': probably the date the house was added to the database, no additional information for houseprice\n",
    "# 'lang', 'lat': Geografical data, not usable in linear regression like this, can be converted to zones using knn. \n",
    "#    Task for later. But zip codes are enough, probably\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57635f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = hp_df.drop(col_drop, axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff110eef",
   "metadata": {},
   "source": [
    "## Changing datatypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6dd6e1",
   "metadata": {},
   "source": [
    "The format of the features are mostly numerical (int or float). Many of the variables have to be transformed to category to give meaningful results in a regression.\n",
    "These are:\n",
    "\n",
    "'zipcode, 'bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cf06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the pairplot we can see which columns are categorical/dummies:\n",
    "#  bderooms, bathrooms, floors, waterfront, view, condition, grade\n",
    "ordinal_var = ['zipcode', 'floors', 'waterfront', 'view', 'condition', 'grade']\n",
    "hp_df[ordinal_var] = hp_df[ordinal_var].astype('category')\n",
    "hp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fafbb",
   "metadata": {},
   "source": [
    "## Checking correlation of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc4753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = hp_df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(9, 7))\n",
    "    ax = sns.heatmap(corr, mask=mask,cmap='coolwarm', vmin=-1,vmax=1,annot=True, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598a819",
   "metadata": {},
   "source": [
    "The features should not be correlated in order to fulfill the asumtions of linear regression.\n",
    "As expected there is a lot of correlation between sqf_above, sqf_living and other measures of size. \n",
    "- First: Drop sqf_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=hp_df.corr().abs()\n",
    "upper_triangle=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(bool))\n",
    "corr_var = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.80)]\n",
    "hp_df = hp_df.drop(corr_var, axis= 1)\n",
    "hp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22505d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = hp_df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(9, 7))\n",
    "    ax = sns.heatmap(corr, mask=mask,cmap='coolwarm', vmin=-1,vmax=1,annot=True, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9cf9b",
   "metadata": {},
   "source": [
    "The new dataframe does not contain anymore variables with a higher correlation than 0.8, but some variable still have high correlation which could be analysed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ceba64",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9e89b",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "To check for the assumtion of normal distributions we check the histograms of the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377572f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_df.select_dtypes(['int64','float']).hist(figsize=(12,12), bins=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d208350",
   "metadata": {},
   "source": [
    "All numerical varibales are non-normaly distributed and need to be transformed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9192c43",
   "metadata": {},
   "source": [
    "## Transforming to normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd80908",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df.select_dtypes(['int64','float']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12465832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables names to be transformed: \n",
    "to_trans =  {'sqft_living': 'qt',\\\n",
    "             'sqft_lot': 'qt',\\\n",
    "             'yr_built': 'qt',\\\n",
    "             'sqft_living15': 'qt',\\\n",
    "             'sqft_lot15': 'qt',\\\n",
    "            'bedrooms': 'qt',\\\n",
    "            'bathrooms': 'qt',\\\n",
    "            'distanceM': 'qt'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1fd3b",
   "metadata": {},
   "source": [
    "For the transformation of the variables the Quantile Transformer is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab4152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pt = PowerTransformer()\n",
    "qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "\n",
    "transformed_cols = []\n",
    "for i in to_trans.keys():\n",
    "    if list(to_trans.values())[0] == 'pt':\n",
    "        transformed = pt.fit_transform(hp_df[i].to_numpy().reshape(-1,1))\n",
    "    elif list(to_trans.values())[0] == 'qt':\n",
    "        transformed = qt.fit_transform(hp_df[i].to_numpy().reshape(-1,1))\n",
    "    else: \n",
    "        print('no transformer could be identified')\n",
    "    col_name = i+'_transformed'\n",
    "    #sns.displot(transformed).set(title=f'Histogram of {col_name}')\n",
    "    hp_df[col_name] = transformed\n",
    "    transformed_cols = transformed_cols + [col_name]\n",
    "    hp_df = hp_df.drop(i, axis= 1)\n",
    "    \n",
    "    \n",
    "hp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816fe0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_df.select_dtypes(['int', 'float64']).hist(figsize=(12,12), bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc6dd2",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e5c3f",
   "metadata": {},
   "source": [
    "The Datasets contains a few datapoints that can described as outliers. One house for example as 33 rooms while the average amount of rooms is around 3. Using the two time the Interquartile range whe remove the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in transformed_cols:\n",
    "    sns.boxplot(data=hp_df[transformed_cols]).set(title='Boxplots of numerical variables showing outliers')\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775308b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " len_before = hp_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-2*iqr\n",
    "    fence_high = q3+2*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "for i in transformed_cols: \n",
    "    hp_df = remove_outlier(hp_df, i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f276cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in transformed_cols:\n",
    "    sns.boxplot(data=hp_df[transformed_cols]).set(title='Boxplots of numerical variables without outliers')\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_after = hp_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaea842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Of the original dataset {round(len_after/len_before * 100, 2)}% of rows remain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36274778",
   "metadata": {},
   "source": [
    "## One-hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_object = list(hp_df.select_dtypes(\"object\").columns)\n",
    "col_names_object.extend(hp_df.select_dtypes(\"category\").columns)\n",
    "col_names_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features =  pd.get_dummies(hp_df[col_names_object] , drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = hp_df.drop(col_names_object, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0253b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df[cat_features.columns] = cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ffbf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(hp_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc28250",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9d3c1",
   "metadata": {},
   "source": [
    "We're splitting into X and y (predictors and target variable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df0d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Xy_split(df, output_col):\n",
    "    \"\"\" Split the dataset into:\n",
    "        X = dataframe of input variables\n",
    "        y = predicted variable\n",
    "    \"\"\"\n",
    "    X=df.drop(output_col, axis=1)\n",
    "    y=df[output_col]\n",
    "    return X, y\n",
    "\n",
    "X,y = Xy_split(hp_df, 'price') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403721c6",
   "metadata": {},
   "source": [
    "We're creating train and test-sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=.30,random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a98085",
   "metadata": {},
   "source": [
    "We're scaling the data using Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler=StandardScaler().fit(X_train.select_dtypes(['int', 'float64']))\n",
    "X_train_scaled=std_scaler.transform(X_train.select_dtypes(['int', 'float64']))\n",
    "X_test_scaled=std_scaler.transform(X_test.select_dtypes(['int', 'float64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78244c26",
   "metadata": {},
   "source": [
    "We're concatenating the data scaled numerical data with the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85e8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled = np.concatenate((X_train_scaled, np.asarray(X_train[list(cat_features.columns)])), axis = 1) \n",
    "\n",
    "X_test_scaled = np.concatenate((X_test_scaled, np.asarray(X_test[list(cat_features.columns)])), axis =1)\n",
    "print(X_train_scaled.shape, X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d807d81",
   "metadata": {},
   "source": [
    "We're modeling using linear regression and desplaying key indicators to interpret the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2860c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=LinearRegression()    # model\n",
    "model.fit(X_train_scaled, y_train) \n",
    "\n",
    "\n",
    "y_pred=model.predict(X_test_scaled)   # model prediction\n",
    "y_pred_train=model.predict(X_train_scaled)\n",
    "\n",
    "\n",
    "result=pd.DataFrame({\"y_test\":y_test,\"y_pred\":y_pred})\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(14,4))\n",
    "ax[0].plot(y_pred, y_test, 'o')\n",
    "ax[0].set_xlabel(\"y_test\")\n",
    "ax[0].set_ylabel(\"y_pred\")\n",
    "ax[0].set_title(\"Test Set -Predicted vs real\")\n",
    "\n",
    "ax[1].hist(y_test - y_pred)\n",
    "ax[1].set_xlabel(\"Test y-y_pred\")\n",
    "ax[1].set_title(\"Test Set Residual histogram\")\n",
    "\n",
    "ax[2].plot(y_pred,y_test - y_pred,\"o\")\n",
    "ax[2].set_xlabel(\"predicted\")\n",
    "ax[2].set_ylabel(\"residuals\")\n",
    "ax[2].set_title(\"Residuals by Predicted\")\n",
    "ax[2].plot(y_pred,np.zeros(len(y_pred)),linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b5769",
   "metadata": {},
   "source": [
    "The scatterplot of the target variable and the predicted variables displays a obvious linear trend. Risiduals seem to be normaly distributed. The Residual are not perfectly randomly distributed around 0, but overall the model seems to be okay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ab2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE test: ',  round(mse(y_test,y_pred),2))\n",
    "print('MSE train: ', round(mse(y_train,y_pred_train),2))\n",
    "print('RMSE test: ', round(np.sqrt(mae(y_test,y_pred)),2))\n",
    "print('MSE train - MSE test: ', round(mse(y_train,y_pred_train) - mse(y_test,y_pred),2) )\n",
    "\n",
    "print('MAE test: ', round(mae(y_test,y_pred),2))\n",
    "\n",
    "R2_test=model.score(X_test_scaled,y_test)\n",
    "R2_train=model.score(X_train_scaled,y_train)\n",
    "print('R2_test: ', round(R2_test, 2))\n",
    "print('R2_train: ', round(R2_train, 2))\n",
    "\n",
    "Adj_R2= 1 - (1-R2_test)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print('Adj_R2:', round(Adj_R2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9153175",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances = pd.DataFrame(data={\n",
    "    'Attribute': X_train.columns,\n",
    "    'Importance': abs(model.coef_)\n",
    "})\n",
    "features_importances = features_importances.sort_values(by='Importance', ascending=False)\n",
    "features_importances\n",
    "plt.figure(figsize=(21,14))\n",
    "plt.bar(x=features_importances['Attribute'], height=features_importances['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances obtained from coefficients', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.figure(figsize=(20,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances.nlargest(10, 'Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b3abc",
   "metadata": {},
   "source": [
    "The Top 10 most important features are mostly zipcodes. Also waterfront and view play an important role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8030c3",
   "metadata": {},
   "source": [
    "## Get P-values from model \n",
    "\n",
    "To filter the significant columns we get the p-value from the linear regression. If it is higher than 0.5 the column is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8758bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_const_scaled = sm.add_constant(X_train_scaled) #getting model using sm.Ols (more detailed statistics)\n",
    "model = sm.OLS(y_train, X_train_const_scaled).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_cols = model.pvalues[1:len(model.pvalues)]  # remove constant\n",
    "sign_cols = pd.DataFrame([sign_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88a05d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_col_list = X.columns\n",
    "X_dropped_p = X\n",
    "for i in range(0,len(X.columns)):\n",
    "    if sign_cols.iloc[0,i]>0.05:\n",
    "        X_dropped_p = X_dropped_p.drop(X_col_list[i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48073746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'From orginally {X.shape[1]} features {X_dropped_p.shape[1]} features remain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1530207",
   "metadata": {},
   "source": [
    "## 3d linear Model only including variabels that had a siginificant p-value in the 2s Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_dropped_p, y, test_size=.30,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(['float64'])\n",
    "X_test_num = X_test.select_dtypes(['float64'])\n",
    "X_train_cat = X_train.select_dtypes(['uint8'])\n",
    "X_test_cat = X_test.select_dtypes(['uint8'])\n",
    "print(X_train_num.shape, X_test_num.shape, X_train_cat.shape, X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler=StandardScaler().fit(X_train_num)   ##. finding the parameters ( mean, variance from the training set )\n",
    "                                            ## \n",
    "X_train_scaled=std_scaler.transform(X_train_num)\n",
    "\n",
    "X_test_scaled=std_scaler.transform(X_test_num)\n",
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = np.concatenate((X_train_scaled, np.asarray(X_train_cat)), axis = 1) \n",
    "\n",
    "X_test_scaled = np.concatenate((X_test_scaled, np.asarray(X_test_cat)), axis =1)\n",
    "print(X_train_scaled.shape, X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c68579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression()    # model\n",
    "model.fit(X_train_scaled, y_train) \n",
    "\n",
    "\n",
    "y_pred=model.predict(X_test_scaled)   # model prediction\n",
    "y_pred_train=model.predict(X_train_scaled)\n",
    "\n",
    "\n",
    "result=pd.DataFrame({\"y_test\":y_test,\"y_pred\":y_pred})\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(14,4))\n",
    "ax[0].plot(y_pred, y_test, 'o')\n",
    "ax[0].set_xlabel(\"y_test\")\n",
    "ax[0].set_ylabel(\"y_pred\")\n",
    "ax[0].set_title(\"Test Set -Predicted vs real\")\n",
    "\n",
    "# Get a histogram of the residuals ie: y - y_pred.  Homoscdasticity\n",
    "# It resembles a normal distribution?\n",
    "ax[1].hist(y_test - y_pred)\n",
    "ax[1].set_xlabel(\"Test y-y_pred\")\n",
    "ax[1].set_title(\"Test Set Residual histogram\")\n",
    "\n",
    "ax[2].plot(y_pred,y_test - y_pred,\"o\")\n",
    "ax[2].set_xlabel(\"predicted\")\n",
    "ax[2].set_ylabel(\"residuals\")\n",
    "ax[2].set_title(\"Residuals by Predicted\")\n",
    "ax[2].plot(y_pred,np.zeros(len(y_pred)),linestyle='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84382604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE test: ',  round(mse(y_test,y_pred),2))\n",
    "print('MSE train: ', round(mse(y_train,y_pred_train),2))\n",
    "print('RMSE test: ', round(np.sqrt(mae(y_test,y_pred)),2))\n",
    "print('MSE train - MSE test: ', round(mse(y_train,y_pred_train) - mse(y_test,y_pred),2) )\n",
    "\n",
    "print('MAE test: ', round(mae(y_test,y_pred),2))\n",
    "\n",
    "R2_test=model.score(X_test_scaled,y_test)\n",
    "R2_train=model.score(X_train_scaled,y_train)\n",
    "print('R2_test: ', round(R2_test, 2))\n",
    "print('R2_train: ', round(R2_train, 2))\n",
    "\n",
    "Adj_R2= 1 - (1-R2_test)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print('Adj_R2:', round(Adj_R2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42908e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances = pd.DataFrame(data={\n",
    "    'Attribute': X_train.columns,\n",
    "    'Importance': abs(model.coef_)\n",
    "})\n",
    "features_importances = features_importances.sort_values(by='Importance', ascending=False)\n",
    "features_importances\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.bar(x=features_importances['Attribute'], height=features_importances['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances obtained from coefficients', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances.nlargest(10, 'Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6555e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_importances.to_csv('Data/final_model_feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066f4cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_const_scaled = sm.add_constant(X_train_scaled) # adding a constant\n",
    "\n",
    "model = sm.OLS(y_train, X_train_const_scaled).fit()\n",
    "predictions_train = model.predict(X_train_const_scaled) \n",
    "\n",
    "X_test_const_scaled = sm.add_constant(X_test_scaled) # adding a constant\n",
    "predictions_test = model.predict(X_test_const_scaled) \n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710ab98",
   "metadata": {},
   "source": [
    "## Visualizing results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01384df3",
   "metadata": {},
   "source": [
    "Zip-code showed to be very determining for prices. The zipcode 98039 showed to show the highest price effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6612bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(features_importances['Importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862df909",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances['zip_rank'] = pd.qcut(features_importances['Importance'], q=4, labels=[\"low\", \"medium\", \"high\", \"premium\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109f500",
   "metadata": {},
   "source": [
    "rank into Quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eba7d8",
   "metadata": {},
   "source": [
    "Get only zipcodes to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_zipcode = features_importances[features_importances['Attribute'].str.startswith('zipcode')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067da2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_zipcode['zipcode']=importance_zipcode['Attribute'].str.replace('zipcode_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_zipcode['zipcode'] =importance_zipcode['zipcode'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8107607",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df_org.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf57f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_result = pd.merge(importance_zipcode[['zipcode', 'zip_rank']], hp_df_org, on='zipcode' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['zip_rank'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "sns.scatterplot(data=df_result, x='long', y='lat', hue='zip_rank').set(title= 'Zipcodes ordered by importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2254c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baa2da3d",
   "metadata": {},
   "source": [
    "Run model again with rated zipcodes to get more precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b223276",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df_org['price_rank'] = pd.qcut(hp_df_org['price'], q=4, labels=[\"low\", \"medium\", \"high\", \"premium\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=hp_df_org, x='long', y='lat', hue='price_rank').set(title= 'Prices ordered geographically')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_zip = hp_df_org[['zipcode', 'price']].groupby('zipcode').mean('price')\n",
    "avg_price_zip['avg_price_zip']=avg_price_zip['price']\n",
    "avg_price_zip.drop('price', axis=1, inplace=True)\n",
    "avg_price_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673fd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_av_price = pd.merge(avg_price_zip[['avg_price_zip']], hp_df_org, on='zipcode')\n",
    "merged_av_price.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c00a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(merged_av_price, x='long', y='lat', hue='avg_price_zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
